# 📚 TFG: Diseño de un sistema de evaluación de grandes modelos de lenguaje basado en técnicas de parafraseo de cuestionarios

Este repositorio contiene el código, datasets y resultados de experimentos realizados durante el TFG, cuyo objetivo es evaluar a los modelos de lenguaje de gran tamaño (LLM) utilizando técnicas de parafraseo de cuestionarios.

## 📂 Estructura del repositorio
📦 TFG_Sandra<br>
├── 📁 Datasets. Conjunto de datos utilizados en los experimentos<br>
├── 📁Resultados. Resultados generados a partir de las pruebas con los modelos <br>
    └──── 📁MMLU<br>
    └──── 📁MMLU pro<br>
├── 📁 Scripts. Código para preprocesamiento, evaluación y experimentos<br>
└── README.md. Documentación del repositorio<br>

## 📄 Descripción del proyecto

El estudio se centra en evaluar cómo la información se ve afectada a través de un proceso de reformulación y parafraseo en modelos de lenguaje. Se analizan las tasas de aciertos antes y después del parafraseo y se analiza como se degrada la información.

## 📊 Resultados obtenidos

- Se han realizado pruebas con preguntas del dataset MMLU y MMLU pro en diferentes categorías.

📍 **Ver detalles en la carpeta [`Resultados/`](./Resultados/).**

## 📁 Datasets utilizados

Los conjuntos de datos empleados en este estudio incluyen:

- **MMLU**: Conjunto de preguntas del Massive Multitask Language Understanding (MMLU). 
- **MMLU pro**: Conjunto de preguntas del Massive Multitask Language Understanding pro(MMLU pro). 

📍 **Disponibles en la carpeta [`Datasets/`](./Datasets/).** 

## 🛠 Requisitos y configuración

⚙️ **Dependencias necesarias** *(se agregarán en el futuro)*  
⚡ **Instrucciones para la ejecución** *(pendiente de añadir)*  
  


---

✉ **Contacto:** Para dudas o colaboración, puedes abrir un *issue* en este repositorio.  
📢 **Actualizaciones:** Se agregarán nuevas secciones conforme el proyecto avance.



