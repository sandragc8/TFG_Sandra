{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado como MMLU/paraphrases_ChatGPT-41-nano.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo original\n",
    "input_file = \"/Users/sandra/Downloads/FinalResults_gpt41nano.xlsx\"\n",
    "output_file = \"MMLU/paraphrases_ChatGPT-41-nano.xlsx\"\n",
    "\n",
    "# Leer el archivo eliminando la primera columna sin nombre\n",
    "df = pd.read_excel(input_file, index_col=0)\n",
    "\n",
    "# Renombrar las columnas correctamente\n",
    "df = df.rename(columns={\n",
    "    df.columns[0]: \"question\", df.columns[1]: \"A\", df.columns[2]: \"B\",\n",
    "    df.columns[3]: \"C\", df.columns[4]: \"D\", df.columns[5]: \"correct_answer\",\n",
    "    df.columns[6]: \"llm_answer_filtered\",\n",
    "    df.columns[7]: \"question\", df.columns[8]: \"A\", df.columns[9]: \"B\",\n",
    "    df.columns[10]: \"C\", df.columns[11]: \"D\",\n",
    "    df.columns[12]: \"llm_answer_filtered\"\n",
    "})\n",
    "\n",
    "# Crear las dos hojas separadas\n",
    "df_en_l0 = df.iloc[:, :7]  # Columnas 0 a 6\n",
    "df_en_l1 = df.iloc[:, [7, 8, 9, 10, 11, 5, 12]]  # Columnas 7 a 12, con correct_answer y llm_answer_filtered correctos\n",
    "\n",
    "# Guardar en un nuevo archivo Excel sin la columna \"Unnamed: 0\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_en_l0.to_excel(writer, sheet_name=\"en_l0\", index=False)\n",
    "    df_en_l1.to_excel(writer, sheet_name=\"en_l1\", index=False)\n",
    "\n",
    "print(f\"Archivo procesado y guardado como {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado como MMLU_pro/paraphrases_ChatGPT-4o-mini.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo original\n",
    "input_file = \"MMLU_pro/ChatGPT-MMLUpro.xlsx\"\n",
    "output_file = \"MMLU_pro/paraphrases_ChatGPT-4o-mini.xlsx\"\n",
    "\n",
    "# Leer el archivo sin eliminar ninguna columna\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Renombrar las columnas correctamente\n",
    "column_names = {\n",
    "    0: \"question\",\n",
    "    1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\", 5: \"E\", 6: \"F\", 7: \"G\", 8: \"H\", 9: \"I\", 10: \"J\",\n",
    "    11: \"correct_answer\",\n",
    "    12: \"llm_answer_filtered\",\n",
    "    13: \"question\",\n",
    "    14: \"A\", 15: \"B\", 16: \"C\", 17: \"D\", 18: \"E\", 19: \"F\", 20: \"G\", 21: \"H\", 22: \"I\", 23: \"J\",\n",
    "    24: \"llm_answer_filtered\"\n",
    "}\n",
    "df = df.rename(columns=column_names)\n",
    "\n",
    "# Crear las dos hojas separadas\n",
    "df_en_l0 = df.iloc[:, :13]  # Columnas 0 a 12\n",
    "df_en_l1 = df.iloc[:, [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 11, 24]]  # Columnas 13 a 24, con correct_answer (col 11)\n",
    "\n",
    "# Guardar en un nuevo archivo Excel\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_en_l0.to_excel(writer, sheet_name=\"en_l0\", index=False)\n",
    "    df_en_l1.to_excel(writer, sheet_name=\"en_l1\", index=False)\n",
    "\n",
    "print(f\"Archivo procesado y guardado como {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         instruction  \\\n",
      "0  Typical advertising regulatory bodies suggest,...   \n",
      "1  Managers are entrusted to run the company in t...   \n",
      "2  There are two main issues associated with ____...   \n",
      "3  _______ locate morality beyond the sphere of r...   \n",
      "4   Some of key differences between Islamic finan...   \n",
      "\n",
      "                                            option_a  \\\n",
      "0            Safe practices, Fear, Jealousy, Trivial   \n",
      "1             Shareholders, Diligence, Self-interest   \n",
      "2              Down, Autonomy, Remuneration, Benefit   \n",
      "3                                     Ethical egoism   \n",
      "4  Interest, Certain, Assured, Both tangible and ...   \n",
      "\n",
      "                                            option_b  \\\n",
      "0           Unsafe practices, Distress, Joy, Trivial   \n",
      "1        Shareholders, Self-interest, Care and Skill   \n",
      "2           Down, Involvement, Independence, Benefit   \n",
      "3                                     Ethics of duty   \n",
      "4  Interest, Uncertain, Assured, Both tangible an...   \n",
      "\n",
      "                                            option_c  \\\n",
      "0           Safe practices, Wants, Jealousy, Trivial   \n",
      "1        Stakeholders, Care and skill, Self-interest   \n",
      "2             Up, Independence, Involvement, Benefit   \n",
      "3                                  Postmodern ethics   \n",
      "4  Interest, Uncertain, Speculative, Intangible a...   \n",
      "\n",
      "                                      option_d  \\\n",
      "0      Safe practices, Distress, Fear, Trivial   \n",
      "1      Stakeholders, Diligence, Care and Skill   \n",
      "2             Down, Privacy, Autonomy, Benefit   \n",
      "3                      Consequentialist ethics   \n",
      "4  Interest, Certain, Assured, Tangible assets   \n",
      "\n",
      "                                          option_e  \\\n",
      "0       Unsafe practices, Wants, Jealousy, Serious   \n",
      "1             Customers, Care and Skill, Diligence   \n",
      "2          Up, Involvement, Autonomy, Compensation   \n",
      "3                               Utilitarian ethics   \n",
      "4  Interest, Uncertain, Assured, Intangible assets   \n",
      "\n",
      "                                          option_f  \\\n",
      "0      Safe practices, Distress, Jealousy, Serious   \n",
      "1          Shareholders, Care and Skill, Diligence   \n",
      "2       Down, Independence, Autonomy, Compensation   \n",
      "3                             Deontological ethics   \n",
      "4  Profit, Uncertain, Speculative, Tangible assets   \n",
      "\n",
      "                                            option_g  \\\n",
      "0               Safe practices, Wants, Fear, Serious   \n",
      "1             Shareholders, Self-interest, Diligence   \n",
      "2           Up, Involvement, Remuneration, Severance   \n",
      "3                                      Virtue ethics   \n",
      "4  Interest, Uncertain, Speculative, Tangible assets   \n",
      "\n",
      "                                            option_h  \\\n",
      "0             Unsafe practices, Wants, Fear, Trivial   \n",
      "1               Employees, Care and Skill, Diligence   \n",
      "2               Up, Privacy, Remuneration, Severance   \n",
      "3                                     Ethics of care   \n",
      "4  Interest, Certain, Speculative, Intangible assets   \n",
      "\n",
      "                                    option_i  \\\n",
      "0  Unsafe practices, Distress, Fear, Serious   \n",
      "1     Stakeholders, Self-interest, Diligence   \n",
      "2   Up, Autonomy, Remuneration, Compensation   \n",
      "3                           Ethics of rights   \n",
      "4  Profit, Certain, Assured, Tangible assets   \n",
      "\n",
      "                                            option_j answer        id  \n",
      "0                                                  -      I  business  \n",
      "1             Stakeholder, Care and Skill, Diligence      F  business  \n",
      "2      Down, Involvement, Remuneration, Compensation      J  business  \n",
      "3                                  Relativist ethics      C  business  \n",
      "4  Interest, Certain, Speculative, Both tangible ...      G  business  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m archivo_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Datasets/MMLU_pro_completo.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Reemplaza con la ruta correcta\u001b[39;00m\n\u001b[1;32m     47\u001b[0m output_excel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mprocesar_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchivo_excel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchivo_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_excel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mprocesar_excel\u001b[0;34m(archivo_excel, archivo_ids, output_excel)\u001b[0m\n\u001b[1;32m     32\u001b[0m datos_extraidos \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_paraphrase\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mapply(extraer_datos)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Crear DataFrame con los datos extraídos\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m df_limpio \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos_extraidos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumnas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Añadir la columna ID desde el otro archivo\u001b[39;00m\n\u001b[1;32m     38\u001b[0m df_limpio[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_ids\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Copia la primera columna de df_ids\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:835\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, columns\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 835\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_list_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:856\u001b[0m, in \u001b[0;36m_list_to_arrays\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    853\u001b[0m     content \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mto_object_array_tuples(data)\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_object_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[0;32mlib.pyx:3014\u001b[0m, in \u001b[0;36mpandas._libs.lib.to_object_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extraer_datos(json_str):\n",
    "    \"\"\"Extrae la pregunta y opciones de respuesta de un JSON en formato string.\"\"\"\n",
    "    try:\n",
    "        # Extraemos solo el JSON ignorando cualquier otro texto adicional\n",
    "        match = re.search(r'\\{.*\\}', json_str)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        question = data.get(\"question\", \"\")\n",
    "        \n",
    "        options = {chr(65+i): data.get(chr(65+i), \"-\") for i in range(10)}  # A-J\n",
    "        \n",
    "        return [question] + list(options.values())\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def procesar_excel(archivo_excel, archivo_ids, output_excel):\n",
    "    # Cargar archivo principal\n",
    "    df = pd.read_excel(archivo_excel, sheet_name='en_l1')\n",
    "    df_ids = pd.read_excel(archivo_ids)  # Archivo con IDs\n",
    "    \n",
    "    print(df_ids.head())  # Verifica las primeras filas del archivo de IDs    \n",
    "    # Extraer datos\n",
    "    columnas = ['instruction', 'option_a', 'option_b', 'option_c', 'option_d', 'option_e', 'option_f', 'option_g', 'option_h', 'option_i', 'option_j']\n",
    "    datos_extraidos = df['raw_paraphrase'].dropna().apply(extraer_datos)\n",
    "    \n",
    "    # Crear DataFrame con los datos extraídos\n",
    "    df_limpio = pd.DataFrame(datos_extraidos.tolist(), columns=columnas)\n",
    "    \n",
    "    # Añadir la columna ID desde el otro archivo\n",
    "    df_limpio['id'] = df_ids.iloc[:, 0]  # Copia la primera columna de df_ids\n",
    "    \n",
    "    # Guardar el resultado en un nuevo archivo Excel\n",
    "    df_limpio.to_excel(output_excel, index=False)\n",
    "    print(f\"Archivo guardado como {output_excel}\")\n",
    "\n",
    "# Parámetros\n",
    "archivo_excel = \"paraphrases_gemma-2-9b-it_16bits_EN-4.xlsx\"  # Reemplaza con la ruta correcta\n",
    "archivo_ids = \"../Datasets/MMLU_pro_completo.xlsx\"  # Reemplaza con la ruta correcta\n",
    "output_excel = \"gemma.xlsx\"\n",
    "\n",
    "procesar_excel(archivo_excel, archivo_ids, output_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado. Archivo guardado como 'resultado.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Elimina caracteres fuera del rango ASCII imprimible y espacios adicionales.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'[^\\x20-\\x7E]', '', text)  # Elimina caracteres no imprimibles\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Reemplaza múltiples espacios por uno solo\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "def fix_json_format(json_text):\n",
    "    \"\"\"Corrige errores comunes en el JSON.\"\"\"\n",
    "    json_text = json_text.replace(\"'\", '\"')  # Reemplazar comillas simples por dobles\n",
    "    json_text = re.sub(r'(\\w)\\s*:\\s*', r'\\1: ', json_text)  # Eliminar espacios innecesarios\n",
    "    json_text = re.sub(r'(?<!\")(\\b\\w+\\b)(?=\\s*:)', r'\"\\1\"', json_text)  # Asegurar comillas en claves\n",
    "    return json_text\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"Extrae el contenido JSON eliminando cualquier otro texto alrededor.\"\"\"\n",
    "    # Buscar el bloque JSON dentro del texto\n",
    "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None  # Retorna None si no encuentra un JSON válido\n",
    "\n",
    "archivo_excel = \"paraphrases_gemma-2-9b-it_16bits_EN-4.xlsx\"\n",
    "archivo_ids = \"../Datasets/MMLU_pro_completo.xlsx\"\n",
    "\n",
    "# Leer los archivos\n",
    "df = pd.read_excel(archivo_excel, sheet_name=\"en_l1\")\n",
    "df_ids = pd.read_excel(archivo_ids)\n",
    "\n",
    "# Verificar que los archivos tengan la misma cantidad de filas\n",
    "if len(df) != len(df_ids):\n",
    "    raise ValueError(\"Los archivos tienen diferente cantidad de filas.\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Recorrer cada fila del dataframe\n",
    "for i, row in df.iterrows():\n",
    "    raw_text = row.get(\"raw_paraphrase\", \"\")\n",
    "    answer = row.get(\"answer\", \"-\")  # Respuesta correcta\n",
    "\n",
    "    # Extraer solo el JSON desde el texto\n",
    "    json_text = extract_json(raw_text)\n",
    "    \n",
    "    if not json_text:\n",
    "        # Si no hay JSON válido, se agrega un registro con valores por defecto\n",
    "        data.append([\"Pregunta no válida\"] + [\"-\"] * 10 + [df_ids.loc[i, 'id'], answer])\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Intentar cargar el JSON\n",
    "        parsed_json = json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Intentar limpiar y corregir el JSON\n",
    "        json_text = clean_text(json_text)\n",
    "        json_text = fix_json_format(json_text)\n",
    "        try:\n",
    "            parsed_json = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Si sigue fallando, marcar la pregunta como inválida\n",
    "            question = \"Pregunta no válida\"\n",
    "            options = {key.lower(): \"-\" for key in \"ABCDEFGHIJ\"}\n",
    "            data.append([question] + list(options.values()) + [df_ids.loc[i, 'id'], answer])\n",
    "            continue\n",
    "\n",
    "    # Extraer pregunta y opciones\n",
    "    question = clean_text(parsed_json.get(\"question\", \"Pregunta no válida\").strip())\n",
    "    options = {key.lower(): clean_text(parsed_json.get(key, \"-\")) for key in \"ABCDEFGHIJ\"}\n",
    "\n",
    "    # Agregar datos a la lista\n",
    "    data.append([question] + [options[f\"{chr(97 + j)}\"] for j in range(10)] + [df_ids.loc[i, 'id']] + [df_ids.loc[i, 'answer']])\n",
    "\n",
    "# Crear DataFrame con las columnas especificadas\n",
    "columnas = [\"instruction\", \"option_a\", \"option_b\", \"option_c\", \"option_d\", \n",
    "            \"option_e\", \"option_f\", \"option_g\", \"option_h\", \"option_i\", \"option_j\", \"id\", \"answer\"]\n",
    "\n",
    "df_final = pd.DataFrame(data, columns=columnas)\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo Excel\n",
    "df_final.to_excel(\"resultado.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Proceso completado. Archivo guardado como 'resultado.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo final guardado como MMLU/paraphrases_ChatGPT-41-nano.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo procesado previamente\n",
    "input_file = \"MMLU/paraphrases_ChatGPT-41-nano.xlsx\"\n",
    "output_file = \"MMLU/paraphrases_ChatGPT-41-nano.xlsx\"\n",
    "\n",
    "# Cargar las hojas en_l0 y en_l1\n",
    "df_en_l0 = pd.read_excel(input_file, sheet_name=\"en_l0\")\n",
    "df_en_l1 = pd.read_excel(input_file, sheet_name=\"en_l1\")\n",
    "\n",
    "# Función para calcular estadísticas\n",
    "def calcular_estadisticas(df, experiment_name):\n",
    "    num_correct = (df[\"correct_answer\"] == df[\"llm_answer_filtered\"]).sum()\n",
    "    total_questions = len(df)\n",
    "    num_wrong = total_questions - num_correct\n",
    "    num_null = df[\"llm_answer_filtered\"].isna().sum()\n",
    "    accuracy = (num_correct / total_questions) * 100 if total_questions > 0 else 0\n",
    "    \n",
    "    return [experiment_name, num_correct, accuracy, num_wrong, num_null]\n",
    "\n",
    "# Calcular estadísticas para en_l0 y en_l1\n",
    "stats_l0 = calcular_estadisticas(df_en_l0, \"en_l0_eval\")\n",
    "stats_l1 = calcular_estadisticas(df_en_l1, \"en_l1_eval\")\n",
    "\n",
    "# Crear DataFrame de estadísticas\n",
    "df_eval_stats = pd.DataFrame([stats_l0, stats_l1], columns=[\n",
    "    \"Experiment\", \"Number of correct answers\", \"Accuracy (%)\", \n",
    "    \"Number of wrong answers\", \"Number of null answers\"\n",
    "])\n",
    "\n",
    "# Guardar todo en un nuevo archivo Excel\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_en_l0.to_excel(writer, sheet_name=\"en_l0\", index=False)\n",
    "    df_en_l1.to_excel(writer, sheet_name=\"en_l1\", index=False)\n",
    "    df_eval_stats.to_excel(writer, sheet_name=\"eval_stats\", index=False)\n",
    "\n",
    "print(f\"Archivo final guardado como {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
